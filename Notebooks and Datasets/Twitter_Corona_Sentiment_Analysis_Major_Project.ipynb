{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using NLP on Coronavirus Tweets Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "import pickle\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alertme(times, diff):\n",
    "    frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "    duration = 500  # Set Duration To 1000 ms == 1 second\n",
    "    for i in range(times*diff):\n",
    "        if i % diff == 0:\n",
    "            winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./Data/Corona_NLP_train.csv', encoding='latin1').drop(columns=['UserName', 'ScreenName', 'Location', 'TweetAt'])\n",
    "df_test = pd.read_csv('./Data/Corona_NLP_test.csv', encoding='latin1').drop(columns=['UserName', 'ScreenName', 'Location', 'TweetAt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "            \n",
    "def strip_accents(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError: # unicode is a default on python 3 \n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def clean_up_sentence(text):\n",
    "    \n",
    "    # Shift to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing mentions, hashtags and urls\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == '#' or text[i] == '@':\n",
    "            j = 0\n",
    "            maxj = len(text)-i\n",
    "            while(j <maxj and text[i+j] != ' '):\n",
    "                if i+j < len(text):\n",
    "                    text = text[0:i+j] + '.' + text[i+j+1:]\n",
    "                    j += 1\n",
    "        elif text[i] == 'h' and i < len(text)-4:\n",
    "            if text[i:i+4] == 'http':\n",
    "                j = 0\n",
    "                maxj = len(text)-i\n",
    "                while(j <maxj and text[i+j] != ' '):\n",
    "                    if i+j < len(text):\n",
    "                        text = text[0:i+j] + '#' + text[i+j+1:]\n",
    "                        j += 1\n",
    "    \n",
    "    # Removing Punctuations and numbers\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    \n",
    "    # Removing unwanted whitespace and removing accents\n",
    "    text = strip_accents(\" \".join(text.split()))\n",
    "    \n",
    "    # Tokenisation\n",
    "    text = re.split('\\W+', text)\n",
    "    if '' in text:\n",
    "        text.remove('')\n",
    "       \n",
    "    # Removing stop words\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    \n",
    "    # Lemmatization\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "\n",
    "    # Remove Stopwords\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train['text']\n",
    "x_test = df_test['text']\n",
    "y_train = df_train['Sentiment']\n",
    "y_test = df_test['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == 'Extremely Negative':\n",
    "        y_train[i] = 'Negative'\n",
    "    elif y_train[i] == 'Extremely Positive':\n",
    "        y_train[i] = 'Positive'\n",
    "        \n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 'Extremely Negative':\n",
    "        y_test[i] = 'Negative'\n",
    "    elif y_test[i] == 'Extremely Positive':\n",
    "        y_test[i] = 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a pipeline to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multinomial Naive Bayes\n",
      "\n",
      "[[1058  141  161]\n",
      " [   1    4    0]\n",
      " [ 574  474 1385]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.65      0.78      0.71      1360\n",
      "     Neutral       0.01      0.80      0.01         5\n",
      "    Positive       0.90      0.57      0.70      2433\n",
      "\n",
      "    accuracy                           0.64      3798\n",
      "   macro avg       0.52      0.72      0.47      3798\n",
      "weighted avg       0.81      0.64      0.70      3798\n",
      "\n",
      "0.644286466561348\n",
      "\n",
      "\n",
      "Bernoulli Naive Bayes\n",
      "\n",
      "[[1217  141  281]\n",
      " [  78  307   49]\n",
      " [ 338  171 1216]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.74      0.74      1639\n",
      "     Neutral       0.50      0.71      0.58       434\n",
      "    Positive       0.79      0.70      0.74      1725\n",
      "\n",
      "    accuracy                           0.72      3798\n",
      "   macro avg       0.68      0.72      0.69      3798\n",
      "weighted avg       0.74      0.72      0.73      3798\n",
      "\n",
      "0.7214323328067404\n",
      "\n",
      "\n",
      "Stochastic Gradient Descent\n",
      "\n",
      "[[1300  157  179]\n",
      " [  61  309   26]\n",
      " [ 272  153 1341]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.79      0.80      1636\n",
      "     Neutral       0.50      0.78      0.61       396\n",
      "    Positive       0.87      0.76      0.81      1766\n",
      "\n",
      "    accuracy                           0.78      3798\n",
      "   macro avg       0.72      0.78      0.74      3798\n",
      "weighted avg       0.80      0.78      0.78      3798\n",
      "\n",
      "0.7767245918904687\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "[[1288  128  174]\n",
      " [  90  391   49]\n",
      " [ 255  100 1323]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.81      0.80      1590\n",
      "     Neutral       0.63      0.74      0.68       530\n",
      "    Positive       0.86      0.79      0.82      1678\n",
      "\n",
      "    accuracy                           0.79      3798\n",
      "   macro avg       0.76      0.78      0.77      3798\n",
      "weighted avg       0.80      0.79      0.79      3798\n",
      "\n",
      "0.7904160084254871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "        MultinomialNB(),\n",
    "        BernoulliNB(),\n",
    "        SGDClassifier(max_iter=10000, n_jobs = 6),\n",
    "        LogisticRegression(max_iter=10000, n_jobs = 6),\n",
    "    ]\n",
    "    \n",
    "model_names = [\n",
    "        '\\nMultinomial Naive Bayes\\n',\n",
    "        '\\nBernoulli Naive Bayes\\n',\n",
    "        '\\nStochastic Gradient Descent\\n',\n",
    "        '\\nLogistic Regression\\n',\n",
    "    ]\n",
    "\n",
    "prd = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    pl = Pipeline([\n",
    "            ('CV',CountVectorizer(analyzer=clean_up_sentence)),\n",
    "            ('tfidf',TfidfTransformer()),\n",
    "            ('classifier',models[i])\n",
    "        ])\n",
    "        \n",
    "    print(model_names[i])\n",
    "        \n",
    "    pl.fit(x_train,y_train)\n",
    "    pred = pl.predict(x_test)\n",
    "    prd.append(pred)\n",
    "        \n",
    "    print(confusion_matrix(pred,y_test), end='\\n\\n')\n",
    "    print(classification_report(pred,y_test))\n",
    "    print(accuracy_score(pred,y_test))\n",
    "    print()\n",
    "\n",
    "alertme(5,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best model for this dataset is Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1288  128  174]\n",
      " [  90  391   49]\n",
      " [ 255  100 1323]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.81      0.80      1590\n",
      "     Neutral       0.63      0.74      0.68       530\n",
      "    Positive       0.86      0.79      0.82      1678\n",
      "\n",
      "    accuracy                           0.79      3798\n",
      "   macro avg       0.76      0.78      0.77      3798\n",
      "weighted avg       0.80      0.79      0.79      3798\n",
      "\n",
      "0.7904160084254871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalPipe = Pipeline([\n",
    "            ('CV',CountVectorizer(analyzer=clean_up_sentence)),\n",
    "            ('tfidf',TfidfTransformer()),\n",
    "            ('classifier',LogisticRegression(max_iter=10000))\n",
    "        ])\n",
    "        \n",
    "finalPipe.fit(x_train,y_train)\n",
    "prd = finalPipe.predict(x_test)\n",
    "        \n",
    "print(confusion_matrix(prd,y_test), end='\\n\\n')\n",
    "print(classification_report(prd,y_test))\n",
    "print(accuracy_score(prd,y_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Major_Project_Twitter_Corona.sav'\n",
    "pickle.dump(finalPipe, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have created the Final Pipeline which should be saved. It contains the model with the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
