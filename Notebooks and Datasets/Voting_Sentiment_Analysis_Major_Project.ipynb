{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Voting System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845068</th>\n",
       "      <td>1845068</td>\n",
       "      <td>jesus</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845069</th>\n",
       "      <td>1845069</td>\n",
       "      <td>kya bhai pure saal chutiya banaya modi aur jab...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845070</th>\n",
       "      <td>1845070</td>\n",
       "      <td>downvote karna tha par upvote hogaya</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845071</th>\n",
       "      <td>1845071</td>\n",
       "      <td>haha nice</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845072</th>\n",
       "      <td>1845072</td>\n",
       "      <td>facebook itself now working bjpÃ¢ÂÂ cell</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1845073 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0                                               text  \\\n",
       "0                 0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   \n",
       "1                 1  advice Talk to your neighbours family to excha...   \n",
       "2                 2  Coronavirus Australia: Woolworths to give elde...   \n",
       "3                 3  My food stock is not the only one which is emp...   \n",
       "4                 4  Me, ready to go at supermarket during the #COV...   \n",
       "...             ...                                                ...   \n",
       "1845068     1845068                                              jesus   \n",
       "1845069     1845069  kya bhai pure saal chutiya banaya modi aur jab...   \n",
       "1845070     1845070              downvote karna tha par upvote hogaya    \n",
       "1845071     1845071                                         haha nice    \n",
       "1845072     1845072        facebook itself now working bjpÃ¢ÂÂ cell    \n",
       "\n",
       "        Sentiment  \n",
       "0         Neutral  \n",
       "1        Positive  \n",
       "2        Positive  \n",
       "3        Positive  \n",
       "4        Negative  \n",
       "...           ...  \n",
       "1845068   Neutral  \n",
       "1845069  Positive  \n",
       "1845070   Neutral  \n",
       "1845071  Positive  \n",
       "1845072   Neutral  \n",
       "\n",
       "[1845073 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('All_data_combined.csv', encoding='latin1')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "            \n",
    "def strip_accents(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError: # unicode is a default on python 3 \n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def clean_up_sentence(text):\n",
    "    \n",
    "    # Shift to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing mentions, hashtags and urls\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == '#' or text[i] == '@':\n",
    "            j = 0\n",
    "            maxj = len(text)-i\n",
    "            while(j <maxj and text[i+j] != ' '):\n",
    "                if i+j < len(text):\n",
    "                    text = text[0:i+j] + '.' + text[i+j+1:]\n",
    "                    j += 1\n",
    "        elif text[i] == 'h' and i < len(text)-4:\n",
    "            if text[i:i+4] == 'http':\n",
    "                j = 0\n",
    "                maxj = len(text)-i\n",
    "                while(j <maxj and text[i+j] != ' '):\n",
    "                    if i+j < len(text):\n",
    "                        text = text[0:i+j] + '#' + text[i+j+1:]\n",
    "                        j += 1\n",
    "    \n",
    "    # Removing Punctuations and numbers\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    \n",
    "    # Removing unwanted whitespace and removing accents\n",
    "    text = strip_accents(\" \".join(text.split()))\n",
    "    \n",
    "    # Tokenisation\n",
    "    text = re.split('\\W+', text)\n",
    "    if '' in text:\n",
    "        text.remove('')\n",
    "       \n",
    "    # Removing stop words\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    \n",
    "    # Lemmatization\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "\n",
    "    # Remove Stopwords\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data to select test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text']\n",
    "y = df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading previously saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter Reddit\n",
      "\n",
      "[[19529   179  5104]\n",
      " [39956  6886 35365]\n",
      " [26597   594 50298]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.23      0.79      0.35     24812\n",
      "     Neutral       0.90      0.08      0.15     82207\n",
      "    Positive       0.55      0.65      0.60     77489\n",
      "\n",
      "    accuracy                           0.42    184508\n",
      "   macro avg       0.56      0.51      0.37    184508\n",
      "weighted avg       0.66      0.42      0.37    184508\n",
      "\n",
      "0.41577058989312116\n",
      "\n",
      "\n",
      "Twitter Corona\n",
      "\n",
      "[[35205  1685 12995]\n",
      " [15862  3297 18261]\n",
      " [35015  2677 59511]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.41      0.71      0.52     49885\n",
      "     Neutral       0.43      0.09      0.15     37420\n",
      "    Positive       0.66      0.61      0.63     97203\n",
      "\n",
      "    accuracy                           0.53    184508\n",
      "   macro avg       0.50      0.47      0.43    184508\n",
      "weighted avg       0.54      0.53      0.50    184508\n",
      "\n",
      "0.5312127387430355\n",
      "\n",
      "\n",
      "Twitter India\n",
      "\n",
      "[[25647   205  7575]\n",
      " [31906  6985 29452]\n",
      " [28529   469 53740]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.30      0.77      0.43     33427\n",
      "     Neutral       0.91      0.10      0.18     68343\n",
      "    Positive       0.59      0.65      0.62     82738\n",
      "\n",
      "    accuracy                           0.47    184508\n",
      "   macro avg       0.60      0.51      0.41    184508\n",
      "weighted avg       0.66      0.47      0.42    184508\n",
      "\n",
      "0.4681206234960002\n",
      "\n",
      "\n",
      "Twitter 1.6 M Tweets\n",
      "\n",
      "[[66507  2625 18104]\n",
      " [    0     0     0]\n",
      " [19575  5034 72663]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hites\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.76      0.77     87236\n",
      "     Neutral       0.00      0.00      0.00         0\n",
      "    Positive       0.80      0.75      0.77     97272\n",
      "\n",
      "    accuracy                           0.75    184508\n",
      "   macro avg       0.52      0.50      0.51    184508\n",
      "weighted avg       0.79      0.75      0.77    184508\n",
      "\n",
      "0.7542762373447222\n",
      "\n",
      "\n",
      "All data Combined\n",
      "\n",
      "[[66796   758 15542]\n",
      " [  638  5012   724]\n",
      " [18648  1889 74501]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.78      0.80      0.79     83096\n",
      "     Neutral       0.65      0.79      0.71      6374\n",
      "    Positive       0.82      0.78      0.80     95038\n",
      "\n",
      "    accuracy                           0.79    184508\n",
      "   macro avg       0.75      0.79      0.77    184508\n",
      "weighted avg       0.79      0.79      0.79    184508\n",
      "\n",
      "0.7929683265766254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit = pickle.load(open('Major_Project_Reddit.sav', 'rb'))\n",
    "corona = pickle.load(open('Major_Project_Twitter_Corona.sav', 'rb'))\n",
    "india = pickle.load(open('Major_Project_Twitter_India.sav', 'rb'))\n",
    "mill = pickle.load(open('Major_Project_Twitter_1.6M.sav', 'rb'))\n",
    "comb = pickle.load(open('Major_Project_Twitter_Combined.sav', 'rb'))\n",
    "\n",
    "# Creating list of all models\n",
    "models = [\n",
    "        reddit,\n",
    "        corona,\n",
    "        india,\n",
    "        mill,\n",
    "        comb\n",
    "    ]\n",
    "    \n",
    "# Providing names of all the models\n",
    "model_names = [\n",
    "        '\\nTwitter Reddit\\n',\n",
    "        '\\nTwitter Corona\\n',\n",
    "        '\\nTwitter India\\n',\n",
    "        '\\nTwitter 1.6 M Tweets\\n',\n",
    "        '\\nAll data Combined\\n'\n",
    "    ]\n",
    "\n",
    "# Empty list to store the predictions of all models\n",
    "prd = []\n",
    "\n",
    "# Main predictor loop\n",
    "for i in range(len(models)):\n",
    "    pl = models[i]\n",
    "        \n",
    "    print(model_names[i])\n",
    "    \n",
    "    pred = pl.predict(x_test)\n",
    "    prd.append(pred)\n",
    "        \n",
    "    print(confusion_matrix(pred,y_test), end='\\n\\n')\n",
    "    print(classification_report(pred,y_test))\n",
    "    print(accuracy_score(pred,y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting of all the different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us see how the predictions list looks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Positive', 'Neutral', 'Positive', ..., 'Neutral', 'Neutral',\n",
       "        'Positive'], dtype=object),\n",
       " array(['Positive', 'Neutral', 'Negative', ..., 'Positive', 'Positive',\n",
       "        'Positive'], dtype=object),\n",
       " array(['Positive', 'Neutral', 'Negative', ..., 'Positive', 'Positive',\n",
       "        'Positive'], dtype=object),\n",
       " array(['Positive', 'Positive', 'Negative', ..., 'Negative', 'Positive',\n",
       "        'Positive'], dtype=object),\n",
       " array(['Positive', 'Positive', 'Negative', ..., 'Positive', 'Neutral',\n",
       "        'Positive'], dtype=object)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all predictions of different models have been stored separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's initialise values which will be useful in voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countX(lst, x):\n",
    "    count = 0\n",
    "    for ele in lst:\n",
    "        if (ele == x):\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Positive', 'Negative', 'Neutral']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = list(y_test.unique())\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[90767, 86082, 7659]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resNums = []\n",
    "for i in responses:\n",
    "    resNums.append(countX(y_test, i))\n",
    "resNums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us create a function to get the f-score of a particular class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFscore(allpredictions, allactuals, response):\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    \n",
    "    for i in range(len(allpredictions)):\n",
    "        if allpredictions[i] == allactuals[i] and allactuals[i] == response:\n",
    "            tp += 1\n",
    "            \n",
    "        elif allpredictions[i] == allactuals[i] and allactuals[i] != response:\n",
    "            tn += 1\n",
    "            \n",
    "        elif allpredictions[i] == response and response != allactuals[i]:\n",
    "            fp += 1\n",
    "            \n",
    "        else:\n",
    "            fn += 1\n",
    "    \n",
    "    if tp != 0:\n",
    "        prec = tp/(tp+fp)\n",
    "        rec = tp/(tp+fn)\n",
    "    \n",
    "    if prec == 0 and rec == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2*prec*rec/(prec+rec)\n",
    "            \n",
    "    return(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the received f-scores of all models, for all classes separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\nTwitter Reddit\\n': {'Positive': 0.4827271811162671,\n",
       "  'Negative': 0.2659666469190279,\n",
       "  'Neutral': 0.11328732303996975},\n",
       " '\\nTwitter Corona\\n': {'Positive': 0.579134572809062,\n",
       "  'Negative': 0.4487428698894236,\n",
       "  'Neutral': 0.07083543705486148},\n",
       " '\\nTwitter India\\n': {'Positive': 0.5227219671620885,\n",
       "  'Negative': 0.3432644047380044,\n",
       "  'Neutral': 0.12461420441367992},\n",
       " '\\nTwitter 1.6 M Tweets\\n': {'Positive': 0.7622099609784753,\n",
       "  'Negative': 0.7457948326904099,\n",
       "  'Neutral': 0},\n",
       " '\\nAll data Combined\\n': {'Positive': 0.7959466028493437,\n",
       "  'Negative': 0.7776426006018942,\n",
       "  'Neutral': 0.2078676150384671}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fscore = {}\n",
    "for i in range(len(model_names)):\n",
    "    fscore[model_names[i]] = {}\n",
    "    for j in range(len(responses)):\n",
    "        fscore[model_names[i]][responses[j]] = getFscore(prd[i], np.array(y_test), responses[j])\n",
    "fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now go ahead with the voting-based predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prdf = []\n",
    "\n",
    "for i in range(len(prd[0])):\n",
    "    votes = [0 for res in responses]\n",
    "    \n",
    "    for j in range(len(prd)):\n",
    "        for response in range(len(responses)):\n",
    "            if responses[response] == prd[j][i]:\n",
    "                votes[response] += fscore[model_names[j]][responses[response]]/resNums[response]**(3/5)\n",
    "    max_index = 0\n",
    "    max_list = []\n",
    "    for vote in range(len(votes)):\n",
    "        if votes[vote] > votes[max_index]:\n",
    "            max_index = vote\n",
    "            max_list = []\n",
    "            max_list = [max_index,]\n",
    "        \n",
    "        elif votes[vote] == votes[max_index]:\n",
    "            max_list.append(vote)\n",
    "    \n",
    "    prdf.append(responses[random.choice(max_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56378   743 11625]\n",
      " [ 1301  5027  1223]\n",
      " [28403  1889 77919]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(prdf,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.65      0.82      0.73     68746\n",
      "     Neutral       0.66      0.67      0.66      7551\n",
      "    Positive       0.86      0.72      0.78    108211\n",
      "\n",
      "    accuracy                           0.76    184508\n",
      "   macro avg       0.72      0.74      0.72    184508\n",
      "weighted avg       0.77      0.76      0.76    184508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(prdf,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our given data, the accuracy is from 70-80%. The accuracy is not too good but it isn't too bad either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
